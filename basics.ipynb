{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1943ec82",
   "metadata": {},
   "source": [
    "I am building a project where i code and learn all the fundamentals used in neural networks to big vission models and llm from scrathc without using and performant industry lib like numpy or pytorch. let start from basics of neural net as basic as vector addition and multiplication. lets go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b86dbf",
   "metadata": {},
   "source": [
    "# Basic vector algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab8044",
   "metadata": {},
   "source": [
    "# Building Deep Learning from First Principles üî•\n",
    "\n",
    "We'll implement everything from scratch using only Python's built-in types:\n",
    "- Basic linear algebra (vectors, matrices)\n",
    "- Gradients and backpropagation\n",
    "- Neural networks (feedforward, CNNs, Transformers)\n",
    "- LLMs and vision models\n",
    "\n",
    "Our \"mini-numpy\" module will include:\n",
    "- Vectors (1D lists)\n",
    "- Matrices (lists of lists)\n",
    "- Operations: addition, subtraction, dot product, scalar multiplication, transpose, etc.\n",
    "\n",
    "## Vector Operations\n",
    "\n",
    "A vector is simply a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fad84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"Add two vectors elementwise\"\"\"\n",
    "    assert len(v) == len(w), \"Vectors must be the same length\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"Subtract w from v elementwise\"\"\"\n",
    "    assert len(v) == len(w)\n",
    "    return [v_i - w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    \"\"\"Multiply every element by scalar c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_sum(v):\n",
    "    \"\"\"Sum all corresponding elements of single vector\"\"\"\n",
    "    assert len(v) > 0, \"No vectors provided!\"\n",
    "    # For 1D vectors, we simply sum all vectors element-wise\n",
    "    return sum(v)\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"Dot product of two vectors\"\"\"\n",
    "    assert len(v) == len(w)\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e90948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector addition: [5, 7, 9]\n",
      "Vector subtraction: [-3, -3, -3]\n",
      "Scalar multiplication: [2, 4, 6]\n",
      "Vector sum: 6\n",
      "Dot product: 32\n",
      "Sum of three vectors: [12, 15, 18]\n"
     ]
    }
   ],
   "source": [
    "# Example vectors\n",
    "v = [1, 2, 3]\n",
    "w = [4, 5, 6]\n",
    "c = 2  # scalar\n",
    "\n",
    "# Test vector_add\n",
    "result_add = vector_add(v, w)\n",
    "print(\"Vector addition:\", result_add)  # Should be [5, 7, 9]\n",
    "assert result_add == [5, 7, 9]\n",
    "\n",
    "# Test vector_subtract\n",
    "result_subtract = vector_subtract(v, w)\n",
    "print(\"Vector subtraction:\", result_subtract)  # Should be [-3, -3, -3]\n",
    "assert result_subtract == [-3, -3, -3]\n",
    "\n",
    "# Test scalar_multiply\n",
    "result_scalar = scalar_multiply(c, v)\n",
    "print(\"Scalar multiplication:\", result_scalar)  # Should be [2, 4, 6]\n",
    "assert result_scalar == [2, 4, 6]\n",
    "\n",
    "# Test vector_sum (for a single vector)\n",
    "result_sum = vector_sum(v)\n",
    "print(\"Vector sum:\", result_sum)  # Should be 6\n",
    "assert result_sum == 6\n",
    "\n",
    "# Test dot product\n",
    "result_dot = dot(v, w)\n",
    "print(\"Dot product:\", result_dot)  # Should be 1*4 + 2*5 + 3*6 = 32\n",
    "assert result_dot == 32\n",
    "\n",
    "# More complex example\n",
    "v2 = [7, 8, 9]\n",
    "# Sum of three vectors\n",
    "result_multi_add = vector_add(vector_add(v, w), v2)\n",
    "print(\"Sum of three vectors:\", result_multi_add)  # Should be [12, 15, 18]\n",
    "assert result_multi_add == [12, 15, 18]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce439110",
   "metadata": {},
   "source": [
    "Now let's move to Step 2: Vector Magnitude & Distance, which are essential for understanding geometry in neural networks ‚Äî distances between embeddings, normalization layers, loss functions, etc.\n",
    "\n",
    "üß† Step 2 ‚Äî Vector Magnitude, Norms, and Distance\n",
    "\n",
    "We'll define three key concepts:\n",
    "\n",
    "Magnitude (Length) ‚Äî how long a vector is\n",
    "\n",
    "‚Äñv‚Äñ = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ‚ãØ + v‚Çô¬≤)\n",
    "\n",
    "Squared magnitude ‚Äî used often in optimization (avoids costly square roots).\n",
    "\n",
    "‚Äñv‚Äñ¬≤ = v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ‚ãØ + v‚Çô¬≤\n",
    "\n",
    "Distance between two vectors ‚Äî how far apart they are in space.\n",
    "\n",
    "d(v,w) = ‚àö((v‚ÇÅ - w‚ÇÅ)¬≤ + (v‚ÇÇ - w‚ÇÇ)¬≤ + ‚ãØ + (v‚Çô - w‚Çô)¬≤)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452137e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"v1^2 + v2^2 + ... + vn^2\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def magnitude(v):\n",
    "    \"\"\"Euclidean length of vector\"\"\"\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    \"\"\"Squared Euclidean distance between v and w\"\"\"\n",
    "    return sum((v_i - w_i) ** 2 for v_i, w_i in zip(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    \"\"\"Euclidean distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd63bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äña‚Äñ = 5.0\n",
      "‚Äña - b‚Äñ = 5.0\n",
      "Squared distance = 25\n"
     ]
    }
   ],
   "source": [
    "a = [3, 4]\n",
    "b = [0, 0]\n",
    "\n",
    "print(\"‚Äña‚Äñ =\", magnitude(a))          # should be 5 (3-4-5 triangle)\n",
    "print(\"‚Äña - b‚Äñ =\", distance(a, b))    # also 5\n",
    "print(\"Squared distance =\", squared_distance(a, b))  # should be 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c4387",
   "metadata": {},
   "source": [
    "üß† Step 3 ‚Äî Matrices: The Building Block of Neural Nets\n",
    "\n",
    "A matrix is just a list of lists ‚Äî\n",
    "each inner list represents a row (or a vector).\n",
    "\n",
    "Example:\n",
    "\n",
    "A = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "This represents the matrix:\n",
    "\n",
    "A = [\n",
    "    1  2  3\n",
    "    4  5  6\n",
    "]\n",
    "\n",
    "can be represented as\n",
    "\n",
    "A = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd29cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1056495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(A):\n",
    "    \"\"\"Return (#rows, #cols) of matrix A\"\"\"\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def get_row(A, i):\n",
    "    \"\"\"Return i-th row of matrix A\"\"\"\n",
    "    return A[i]\n",
    "\n",
    "def get_col(A, j):\n",
    "    \"\"\"Return j-th column of matrix A\"\"\"\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"\n",
    "    Create a matrix given a function entry_fn(i, j)\n",
    "    that returns the element at row i, column j.\n",
    "    \"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)] for i in range(num_rows)]\n",
    "\n",
    "def identity_matrix(n):\n",
    "    \"\"\"n√ón Identity matrix\"\"\"\n",
    "    return make_matrix(n, n, lambda i, j: 1 if i == j else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c902901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2, 3)\n",
      "Row 0: [1, 2, 3]\n",
      "Col 1: [2, 5]\n",
      "Identity(3): [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "A = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "\n",
    "print(\"Shape:\", shape(A))             # (2, 3)\n",
    "print(\"Row 0:\", get_row(A, 0))        # [1, 2, 3]\n",
    "print(\"Col 1:\", get_col(A, 1))        # [2, 5]\n",
    "print(\"Identity(3):\", identity_matrix(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81a48a",
   "metadata": {},
   "source": [
    "üß© Step 4 ‚Äî Gradients and Differentiation\n",
    "üß† 4.1 ‚Äî The Core Idea\n",
    "\n",
    "A neural network learns by minimizing a loss function \n",
    "ùêø\n",
    "(\n",
    "ùë§\n",
    ")\n",
    "L(w), where \n",
    "ùë§\n",
    "w are weights.\n",
    "To minimize \n",
    "ùêø\n",
    "L, we need to know how small changes in \n",
    "ùë§\n",
    "w affect \n",
    "ùêø\n",
    "L.\n",
    "That‚Äôs what the derivative or gradient tells us.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "ùëë\n",
    "ùêø\n",
    "ùëë\n",
    "ùë§\n",
    "=\n",
    "slope of \n",
    "ùêø\n",
    "(\n",
    "ùë§\n",
    ")\n",
    "dw\n",
    "dL\n",
    "\t‚Äã\n",
    "\n",
    "=slope of L(w)\n",
    "\n",
    "We can approximate the derivative using a finite difference:\n",
    "\n",
    "ùëë\n",
    "ùëì\n",
    "ùëë\n",
    "ùë•\n",
    "‚âà\n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    "+\n",
    "‚Ñé\n",
    ")\n",
    "‚àí\n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    "‚àí\n",
    "‚Ñé\n",
    ")\n",
    "2\n",
    "‚Ñé\n",
    "dx\n",
    "df\n",
    "\t‚Äã\n",
    "\n",
    "‚âà\n",
    "2h\n",
    "f(x+h)‚àíf(x‚àíh)\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "for very small \n",
    "‚Ñé\n",
    "h (say \n",
    "1\n",
    "ùëí\n",
    "‚àí\n",
    "5\n",
    "1e‚àí5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f3faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_utils.py\n",
    "\n",
    "def derivative(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Numerical derivative of f at x using central difference.\n",
    "    f: function f(x)\n",
    "    x: point at which to differentiate\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6350a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'(0) ‚âà 0.0000 (true=0)\n",
      "f'(1) ‚âà 2.0000 (true=2)\n",
      "f'(2) ‚âà 4.0000 (true=4)\n",
      "f'(3) ‚âà 6.0000 (true=6)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2  # derivative should be 2x\n",
    "\n",
    "for x in [0, 1, 2, 3]:\n",
    "    print(f\"f'({x}) ‚âà {derivative(f, x):.4f} (true={2*x})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a232ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'(0) ‚âà 0.0000 (true=0)\n",
      "f'(1) ‚âà 3.0000 (true=3)\n",
      "f'(2) ‚âà 12.0000 (true=12)\n",
      "f'(3) ‚âà 27.0000 (true=27)\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "for x in [0, 1, 2, 3]:\n",
    "    print(f\"f'({x}) ‚âà {derivative(f2, x):.4f} (true={3*x**2})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3321405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, v, h=1e-5):\n",
    "    \"\"\"\n",
    "    Estimate gradient of f at vector v using finite differences.\n",
    "    f: function f(v) that returns a scalar\n",
    "    v: list of parameters\n",
    "    \"\"\"\n",
    "    grad = []\n",
    "    for i in range(len(v)):\n",
    "        v_step_up = v[:]\n",
    "        v_step_down = v[:]\n",
    "        v_step_up[i] += h\n",
    "        v_step_down[i] -= h\n",
    "        grad_i = (f(v_step_up) - f(v_step_down)) / (2 * h)\n",
    "        grad.append(grad_i)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "454008fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚àáf([3, 4]) = [6.000000000128124, 7.999999999874773]\n"
     ]
    }
   ],
   "source": [
    "def f(v):\n",
    "    # f(x, y) = x^2 + y^2\n",
    "    x, y = v\n",
    "    return x**2 + y**2\n",
    "\n",
    "print(\"‚àáf([3, 4]) =\", gradient(f, [3, 4]))  # should be [6, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11748f4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "009d7eda",
   "metadata": {},
   "source": [
    "üß© Step 4.4 ‚Äî Using Gradients to Optimize (Gradient Descent)\n",
    "\n",
    "This is where we first teach our model to move in the direction that reduces loss.\n",
    "\n",
    "Gradient descent rule:\n",
    "\n",
    "ùë•\n",
    "new\n",
    "=\n",
    "ùë•\n",
    "‚àí\n",
    "ùúÇ\n",
    "‚ãÖ\n",
    "‚àá\n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "x\n",
    "new\n",
    "\t‚Äã\n",
    "\n",
    "=x‚àíŒ∑‚ãÖ‚àáf(x)\n",
    "\n",
    "where \n",
    "ùúÇ\n",
    "Œ∑ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b175fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_v, learning_rate=0.1, steps=100):\n",
    "    v = init_v\n",
    "    for step in range(steps):\n",
    "        grad = gradient(f, v)\n",
    "        v = [v_i - learning_rate * g_i for v_i, g_i in zip(v, grad)]\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aae02c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized v: [2.9999571825692226, -0.9999857275230724]\n"
     ]
    }
   ],
   "source": [
    "def f(v):\n",
    "    x, y = v\n",
    "    return (x - 3)**2 + (y + 1)**2\n",
    "\n",
    "opt_v = gradient_descent(f, init_v=[0, 0], learning_rate=0.1, steps=50)\n",
    "print(\"Optimized v:\", opt_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcca680",
   "metadata": {},
   "source": [
    "üß† Step 5 ‚Äî Building a Single Neuron (Forward + Backward)\n",
    "\n",
    "A neuron is the smallest building block of a neural network.\n",
    "It takes inputs, multiplies by weights, adds a bias, and applies an activation function.\n",
    "\n",
    "üß© 5.1 ‚Äî Neuron formula\n",
    "\n",
    "Given:\n",
    "\n",
    "inputs: \n",
    "ùë•\n",
    "1\n",
    ",\n",
    "ùë•\n",
    "2\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "ùë•\n",
    "ùëõ\n",
    "weights: \n",
    "ùë§\n",
    "1\n",
    ",\n",
    "ùë§\n",
    "2\n",
    ",\n",
    "‚Ä¶\n",
    ",\n",
    "ùë§\n",
    "ùëõ\n",
    "bias: \n",
    "ùëè\n",
    "inputs: x\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    ",x\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    ",‚Ä¶,x\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "weights: w\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    ",w\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    ",‚Ä¶,w\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "bias: b\n",
    "\n",
    "Then:\n",
    "\n",
    "ùë¶\n",
    "=\n",
    "ùëì\n",
    "(\n",
    "ùë§\n",
    "1\n",
    "ùë•\n",
    "1\n",
    "+\n",
    "ùë§\n",
    "2\n",
    "ùë•\n",
    "2\n",
    "+\n",
    "‚Ä¶\n",
    "+\n",
    "ùë§\n",
    "ùëõ\n",
    "ùë•\n",
    "ùëõ\n",
    "+\n",
    "ùëè\n",
    ")\n",
    "y=f(w\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    "x\n",
    "1\n",
    "\t‚Äã\n",
    "\n",
    "+w\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    "x\n",
    "2\n",
    "\t‚Äã\n",
    "\n",
    "+‚Ä¶+w\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "x\n",
    "n\n",
    "\t‚Äã\n",
    "\n",
    "+b)\n",
    "\n",
    "where \n",
    "ùëì\n",
    "f is an activation function (e.g. sigmoid, ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03b1f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35723b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid squashes values to (0, 1)\"\"\"\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return x if x > 0 else 0\n",
    "\n",
    "def relu_prime(x):\n",
    "    return 1 if x > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f3ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def neuron_output(weights, inputs, bias, activation=sigmoid):\n",
    "    \"\"\"Compute output of a single neuron\"\"\"\n",
    "    z = dot(weights, inputs) + bias\n",
    "    return activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c93a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: 0.598687660112452\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [0.5, -0.2, 0.1]\n",
    "bias = 0.0\n",
    "\n",
    "print(\"Neuron output:\", neuron_output(weights, inputs, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d85a33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_single_neuron(data, learning_rate=0.1, steps=1000):\n",
    "    \"\"\"\n",
    "    data: list of (x, y_true) pairs\n",
    "    learning_rate: step size for gradient descent\n",
    "    steps: number of iterations\n",
    "    \"\"\"\n",
    "    # initialize weight and bias randomly\n",
    "    w = random.uniform(-1, 1)\n",
    "    b = random.uniform(-1, 1)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        dw, db = 0.0, 0.0  # gradients\n",
    "        loss = 0.0\n",
    "        for x, y_true in data:\n",
    "            # forward\n",
    "            z = w * x + b\n",
    "            y_pred = sigmoid(z)\n",
    "            \n",
    "            # loss (Mean Squared Error)\n",
    "            error = y_pred - y_true\n",
    "            loss += error ** 2\n",
    "            \n",
    "            # backprop: derivative of loss wrt w and b\n",
    "            dL_dy = 2 * error\n",
    "            dy_dz = sigmoid_prime(z)\n",
    "            dz_dw = x\n",
    "            dz_db = 1\n",
    "            \n",
    "            dw += dL_dy * dy_dz * dz_dw\n",
    "            db += dL_dy * dy_dz * dz_db\n",
    "        \n",
    "        # average gradients\n",
    "        dw /= len(data)\n",
    "        db /= len(data)\n",
    "        loss /= len(data)\n",
    "        \n",
    "        # update weights\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # monitor progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: loss={loss:.6f}, w={w:.3f}, b={b:.3f}\")\n",
    "    \n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80e75f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=5.389308, w=-0.599, b=0.000\n",
      "Step 100: loss=2.878920, w=3.310, b=0.072\n",
      "Step 200: loss=2.840758, w=4.436, b=-0.670\n",
      "Step 300: loss=2.827483, w=5.175, b=-0.993\n",
      "Step 400: loss=2.820845, w=5.712, b=-1.191\n",
      "Step 500: loss=2.816840, w=6.134, b=-1.334\n",
      "Step 600: loss=2.814150, w=6.483, b=-1.446\n",
      "Step 700: loss=2.812213, w=6.779, b=-1.538\n",
      "Step 800: loss=2.810748, w=7.038, b=-1.616\n",
      "Step 900: loss=2.809601, w=7.268, b=-1.683\n",
      "Learned weight: 7.471839744996861\n",
      "Learned bias: -1.7427369012286837\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for y = 2x\n",
    "data = [(x, 2 * x) for x in [0.0, 0.5, 1.0, 1.5, 2.0]]\n",
    "\n",
    "w, b = train_single_neuron(data, learning_rate=0.5, steps=1000)\n",
    "print(\"Learned weight:\", w)\n",
    "print(\"Learned bias:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70acdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs, activation=sigmoid):\n",
    "        self.weights = [random.uniform(-1, 1) for _ in range(num_inputs)]\n",
    "        self.bias = random.uniform(-1, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Compute neuron output and store intermediate z for backprop\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = dot(self.weights, inputs) + self.bias\n",
    "        self.output = self.activation(self.z)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd69ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_neurons, activation=sigmoid):\n",
    "        self.neurons = [Neuron(num_inputs, activation) for _ in range(num_neurons)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Feed inputs to all neurons and collect outputs\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = [neuron.forward(inputs) for neuron in self.neurons]\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7c48f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        layer_sizes: e.g. [2, 3, 1]\n",
    "        means 2 inputs ‚Üí hidden layer of 3 neurons ‚Üí 1 output neuron\n",
    "        \"\"\"\n",
    "        self.layers = [\n",
    "            Layer(num_inputs=layer_sizes[i], num_neurons=layer_sizes[i + 1])\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84901790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.34180279008819]\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork([2, 5, 1])\n",
    "output = net.forward([1.0, 0.5])\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9268b",
   "metadata": {},
   "source": [
    "üß© 6.4 ‚Äî Backpropagation (manual)\n",
    "\n",
    "We‚Äôll now make the network learn.\n",
    "We‚Äôll implement gradient descent on all weights and biases via backpropagation.\n",
    "\n",
    "‚öôÔ∏è Key idea\n",
    "\n",
    "Compute forward pass.\n",
    "\n",
    "Compute loss (e.g., Mean Squared Error).\n",
    "\n",
    "Compute output error (dL/dz for last layer).\n",
    "\n",
    "Propagate error backward through layers.\n",
    "\n",
    "Update weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "487f592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation=sigmoid, activation_prime=sigmoid_prime):\n",
    "        self.layers = [\n",
    "            Layer(layer_sizes[i], layer_sizes[i + 1], activation)\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ]\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, target, learning_rate=0.1):\n",
    "        # Compute output layer error\n",
    "        output_layer = self.layers[-1]\n",
    "        errors = []\n",
    "        for i, neuron in enumerate(output_layer.neurons):\n",
    "            dL_dy = 2 * (neuron.output - target[i])  # MSE derivative\n",
    "            dy_dz = self.activation_prime(neuron.z)\n",
    "            errors.append(dL_dy * dy_dz)\n",
    "\n",
    "        # Propagate backwards through layers\n",
    "        for l in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[l]\n",
    "            new_errors = [0.0] * len(layer.neurons[0].inputs)\n",
    "\n",
    "            for j, neuron in enumerate(layer.neurons):\n",
    "                error = errors[j]\n",
    "                # FIRST: accumulate errors for previous layer using OLD weights (before update)\n",
    "                for k in range(len(neuron.inputs)):\n",
    "                    new_errors[k] += neuron.weights[k] * error\n",
    "                \n",
    "                # THEN: update weights and bias using gradients\n",
    "                for k, x_k in enumerate(neuron.inputs):\n",
    "                    grad_w = error * x_k\n",
    "                    neuron.weights[k] -= learning_rate * grad_w\n",
    "                neuron.bias -= learning_rate * error\n",
    "\n",
    "            # Apply activation derivative for hidden layers\n",
    "            if l > 0:\n",
    "                prev_layer = self.layers[l - 1]\n",
    "                for k, prev_neuron in enumerate(prev_layer.neurons):\n",
    "                    new_errors[k] *= self.activation_prime(prev_neuron.z)\n",
    "            \n",
    "            errors = new_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed2d53",
   "metadata": {},
   "source": [
    "We‚Äôll train on the XOR problem, a classic nonlinear dataset that can‚Äôt be solved by one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ba64713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=1.253331\n",
      "Epoch 1000: loss=0.003451\n",
      "Epoch 2000: loss=0.001467\n",
      "Epoch 3000: loss=0.000923\n",
      "Epoch 4000: loss=0.000672\n",
      "Epoch 5000: loss=0.000527\n",
      "Epoch 6000: loss=0.000433\n",
      "Epoch 7000: loss=0.000367\n",
      "Epoch 8000: loss=0.000319\n",
      "Epoch 9000: loss=0.000282\n",
      "\n",
      "Final outputs:\n",
      "[0, 0] -> [0.007945477925819214]\n",
      "[0, 1] -> [0.9925443378711539]\n",
      "[1, 0] -> [0.9908896502550989]\n",
      "[1, 1] -> [0.007089798656080845]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ([0, 0], [0]),\n",
    "    ([0, 1], [1]),\n",
    "    ([1, 0], [1]),\n",
    "    ([1, 1], [0]),\n",
    "]\n",
    "\n",
    "net = NeuralNetwork([2, 2, 1])  # 2 inputs ‚Üí 2 hidden ‚Üí 1 output\n",
    "\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "    for x, y_true in data:\n",
    "        y_pred = net.forward(x)\n",
    "        total_loss += sum((y_true[i] - y_pred[i])**2 for i in range(len(y_true)))\n",
    "        net.backward(y_true, learning_rate=1.0)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: loss={total_loss:.6f}\")\n",
    "\n",
    "print(\"\\nFinal outputs:\")\n",
    "for x, y_true in data:\n",
    "    print(x, \"->\", net.forward(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f514e11",
   "metadata": {},
   "source": [
    "\n",
    "## Detailed explanation of your NeuralNetwork class (focus: backward)\n",
    "\n",
    "I'll walk through the __init__, forward, then go line-by-line through backward with the math.\n",
    "\n",
    "High-level recap\n",
    "\n",
    "layer_sizes is something like [input_dim, hidden_dim, ..., output_dim].\n",
    "\n",
    "self.layers is a list of Layer objects. Each Layer contains Neuron objects that expose:\n",
    "\n",
    "neuron.weights ‚Äî list of weights (length = number of inputs to that neuron).\n",
    "\n",
    "neuron.bias ‚Äî scalar bias.\n",
    "\n",
    "neuron.inputs ‚Äî the input vector that was used on the most recent forward pass (stored for backprop).\n",
    "\n",
    "neuron.z ‚Äî pre-activation scalar = dot(weights, inputs) + bias (stored).\n",
    "\n",
    "neuron.output ‚Äî post-activation value (activation(z)) (stored).\n",
    "\n",
    "__init__ and forward\n",
    "def __init__(..., activation=sigmoid, activation_prime=sigmoid_prime):\n",
    "    self.layers = [...]\n",
    "    self.activation_prime = activation_prime\n",
    "\n",
    "\n",
    "activation_prime is the derivative function for the activation (i.e., \n",
    "ùëì\n",
    "‚Ä≤\n",
    "(\n",
    "ùëß\n",
    ")\n",
    "f\n",
    "‚Ä≤\n",
    "(z)). You need f'(z) for backprop.\n",
    "\n",
    "def forward(self, inputs):\n",
    "    for layer in self.layers:\n",
    "        inputs = layer.forward(inputs)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "Chains forward through layers. Each layer.forward should set each neuron's .inputs, .z, and .output. The final returned inputs is the network output (list).\n",
    "\n",
    "backward(self, target, learning_rate=0.1) ‚Äî detailed walkthrough\n",
    "1) Compute output-layer deltas (errors)\n",
    "output_layer = self.layers[-1]\n",
    "errors = []\n",
    "for i, neuron in enumerate(output_layer.neurons):\n",
    "    dL_dy = 2 * (neuron.output - target[i])  # MSE derivative\n",
    "    dy_dz = self.activation_prime(neuron.z)\n",
    "    errors.append(dL_dy * dy_dz)\n",
    "\n",
    "\n",
    "Math:\n",
    "\n",
    "Loss used: Mean Squared Error per sample (not averaged across batch here). For a single output neuron with MSE \n",
    "ùêø\n",
    "=\n",
    "(\n",
    "ùë¶\n",
    "‚àí\n",
    "ùë°\n",
    ")\n",
    "2\n",
    "L=(y‚àít)\n",
    "2\n",
    ", derivative w.r.t. output \n",
    "ùë¶\n",
    "y is \n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë¶\n",
    "=\n",
    "2\n",
    "(\n",
    "ùë¶\n",
    "‚àí\n",
    "ùë°\n",
    ")\n",
    "‚àÇy\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "=2(y‚àít). You correctly compute dL_dy.\n",
    "\n",
    "For the neuron we want \n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëß\n",
    "‚àÇz\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    " (where \n",
    "ùëß\n",
    "z is pre-activation), and \n",
    "‚àÇ\n",
    "ùë¶\n",
    "‚àÇ\n",
    "ùëß\n",
    "=\n",
    "ùëì\n",
    "‚Ä≤\n",
    "(\n",
    "ùëß\n",
    ")\n",
    "‚àÇz\n",
    "‚àÇy\n",
    "\t‚Äã\n",
    "\n",
    "=f\n",
    "‚Ä≤\n",
    "(z). So:\n",
    "\n",
    "ùõø\n",
    "=\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëß\n",
    "=\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë¶\n",
    "‚ãÖ\n",
    "ùëì\n",
    "‚Ä≤\n",
    "(\n",
    "ùëß\n",
    ")\n",
    "=\n",
    "2\n",
    "(\n",
    "ùë¶\n",
    "‚àí\n",
    "ùë°\n",
    ")\n",
    "‚ãÖ\n",
    "ùëì\n",
    "‚Ä≤\n",
    "(\n",
    "ùëß\n",
    ")\n",
    "Œ¥=\n",
    "‚àÇz\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "=\n",
    "‚àÇy\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖf\n",
    "‚Ä≤\n",
    "(z)=2(y‚àít)‚ãÖf\n",
    "‚Ä≤\n",
    "(z)\n",
    "\n",
    "errors is a list of \n",
    "ùõø\n",
    "ùëó\n",
    "Œ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    " for each output neuron \n",
    "ùëó\n",
    "j. Each error represents \n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëó\n",
    "‚àÇL/‚àÇz\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    ".\n",
    "\n",
    "2) Loop layers in reverse (backpropagate)\n",
    "for l in reversed(range(len(self.layers))):\n",
    "    layer = self.layers[l]\n",
    "    new_errors = [0.0] * len(layer.neurons[0].inputs)\n",
    "\n",
    "\n",
    "Iterate layers from last to first.\n",
    "\n",
    "new_errors will accumulate errors for the previous layer‚Äôs neurons. Its length equals the number of inputs to each neuron in the current layer, which is also the number of neurons in the previous layer (except for the first layer where inputs are raw features).\n",
    "\n",
    "3) For each neuron: accumulate previous-layer errors before updating weights\n",
    "    for j, neuron in enumerate(layer.neurons):\n",
    "        error = errors[j]\n",
    "        # FIRST: accumulate errors for previous layer using OLD weights (before update)\n",
    "        for k in range(len(neuron.inputs)):\n",
    "            new_errors[k] += neuron.weights[k] * error\n",
    "\n",
    "\n",
    "Math & reasoning:\n",
    "\n",
    "Backpropagate errors to the previous layer using the chain rule. If current layer‚Äôs neuron \n",
    "ùëó\n",
    "j has weight \n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "w\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    " connecting it to previous-layer neuron \n",
    "ùëò\n",
    "k:\n",
    "\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    " receives contribution \n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "‚ãÖ\n",
    "ùõø\n",
    "ùëó\n",
    "‚àÇz\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    " receives contribution w\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖŒ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "(sum over all j connected to k).\n",
    "\n",
    "So new_errors[k] += neuron.weights[k] * error builds the sum:\n",
    "\n",
    "new_errors\n",
    "[\n",
    "ùëò\n",
    "]\n",
    "=\n",
    "‚àë\n",
    "ùëó\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "‚ãÖ\n",
    "ùõø\n",
    "ùëó\n",
    "new_errors[k]=\n",
    "j\n",
    "‚àë\n",
    "\t‚Äã\n",
    "\n",
    "w\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖŒ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "IMPORTANT: this must be done with the old weights (weights that produced the forward pass) ‚Äî hence accumulation happens before you change neuron.weights. Your code correctly follows this order.\n",
    "\n",
    "4) Then update the current neuron's weights & bias using gradient\n",
    "        # THEN: update weights and bias using gradients\n",
    "        for k, x_k in enumerate(neuron.inputs):\n",
    "            grad_w = error * x_k\n",
    "            neuron.weights[k] -= learning_rate * grad_w\n",
    "        neuron.bias -= learning_rate * error\n",
    "\n",
    "\n",
    "Math:\n",
    "\n",
    "For a given neuron \n",
    "ùëó\n",
    "j, with input \n",
    "ùë•\n",
    "ùëò\n",
    "x\n",
    "k\n",
    "\t‚Äã\n",
    "\n",
    ",\n",
    "\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "=\n",
    "ùõø\n",
    "ùëó\n",
    "‚ãÖ\n",
    "ùë•\n",
    "ùëò\n",
    "‚àÇw\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "=Œ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖx\n",
    "k\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "because \n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëó\n",
    "/\n",
    "‚àÇ\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "=\n",
    "ùë•\n",
    "ùëò\n",
    "‚àÇz\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "/‚àÇw\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "=x\n",
    "k\n",
    "\t‚Äã\n",
    "\n",
    " and \n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "=\n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëó\n",
    "‚ãÖ\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëó\n",
    "/\n",
    "‚àÇ\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "‚àÇL/‚àÇw\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "=‚àÇL/‚àÇz\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖ‚àÇz\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "/‚àÇw\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    ".\n",
    "\n",
    "Bias gradient is \n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùëè\n",
    "ùëó\n",
    "=\n",
    "ùõø\n",
    "ùëó\n",
    "‚àÇL/‚àÇb\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "=Œ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    ".\n",
    "\n",
    "You apply gradient descent update \n",
    "ùë§\n",
    "‚Üê\n",
    "ùë§\n",
    "‚àí\n",
    "ùúÇ\n",
    "‚ãÖ\n",
    "grad\n",
    "ùë§\n",
    "w‚Üêw‚àíŒ∑‚ãÖgrad\n",
    "w\n",
    "\t‚Äã\n",
    "\n",
    " ‚Äî correct.\n",
    "\n",
    "Why update after accumulation?\n",
    "\n",
    "If you updated weights first, then new_errors would be computed using changed weights (which correspond to a different forward function) and would break the correctness of backprop. So the order you chose is essential: accumulate, then update.\n",
    "\n",
    "5) Apply activation derivative for previous layer\n",
    "    # Apply activation derivative for hidden layers\n",
    "    if l > 0:\n",
    "        prev_layer = self.layers[l - 1]\n",
    "        for k, prev_neuron in enumerate(prev_layer.neurons):\n",
    "            new_errors[k] *= self.activation_prime(prev_neuron.z)\n",
    "    \n",
    "    errors = new_errors\n",
    "\n",
    "\n",
    "Math:\n",
    "\n",
    "new_errors[k] at this moment is:\n",
    "\n",
    "‚àë\n",
    "ùëó\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "‚ãÖ\n",
    "ùõø\n",
    "ùëó\n",
    "=\n",
    "‚àë\n",
    "ùëó\n",
    "ùë§\n",
    "ùëó\n",
    "ùëò\n",
    "‚ãÖ\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëó\n",
    "j\n",
    "‚àë\n",
    "\t‚Äã\n",
    "\n",
    "w\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖŒ¥\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "=\n",
    "j\n",
    "‚àë\n",
    "\t‚Äã\n",
    "\n",
    "w\n",
    "jk\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖ\n",
    "‚àÇz\n",
    "j\n",
    "\t‚Äã\n",
    "\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "which equals \n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùëé\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    "‚àÇL/‚àÇa\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    " (where \n",
    "ùëé\n",
    "a is activation value).\n",
    "\n",
    "To get \n",
    "‚àÇ\n",
    "ùêø\n",
    "/\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    "‚àÇL/‚àÇz\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    " (the delta for previous layer), multiply by the derivative of the activation at z_k^{(prev)}:\n",
    "\n",
    "ùõø\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    "=\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëß\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    "=\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùëé\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    "‚ãÖ\n",
    "ùëì\n",
    "‚Ä≤\n",
    "(\n",
    "ùëß\n",
    "ùëò\n",
    "(\n",
    "ùëù\n",
    "ùëü\n",
    "ùëí\n",
    "ùë£\n",
    ")\n",
    ")\n",
    "Œ¥\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    "=\n",
    "‚àÇz\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "=\n",
    "‚àÇa\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "‚ãÖf\n",
    "‚Ä≤\n",
    "(z\n",
    "k\n",
    "(prev)\n",
    "\t‚Äã\n",
    "\n",
    ")\n",
    "\n",
    "That is exactly what new_errors[k] *= activation_prime(prev_neuron.z) does.\n",
    "\n",
    "You do if l > 0 because there is no previous layer for the first (input) layer ‚Äî input features are not activated neurons.\n",
    "\n",
    "After this, errors = new_errors prepares for the next iteration (moving backward).\n",
    "\n",
    "Shape / dimensional sanity check (per-sample)\n",
    "\n",
    "Let n_{l-1} = number of neurons in previous layer (or input dimension for first hidden).\n",
    "\n",
    "Let n_l = number of neurons in layer l.\n",
    "\n",
    "For layer l:\n",
    "\n",
    "neuron.weights length = n_{l-1}\n",
    "\n",
    "neuron.inputs length = n_{l-1}\n",
    "\n",
    "errors length = n_l (one delta per neuron in current layer)\n",
    "\n",
    "new_errors length = n_{l-1} (one accumulation per neuron in previous layer)\n",
    "\n",
    "Everything matches: you accumulate contributions from n_l neurons into n_{l-1} previous-neuron error entries.\n",
    "\n",
    "Why this implementation works (intuition)\n",
    "\n",
    "You compute \n",
    "ùõø\n",
    "Œ¥ at the outputs (error w.r.t. pre-activation).\n",
    "\n",
    "For each layer you:\n",
    "\n",
    "propagate error to previous neurons via weight-weighted sum,\n",
    "\n",
    "convert that to previous-layer deltas by multiplying by activation derivatives,\n",
    "\n",
    "update the current layer's weights using \n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë§\n",
    "=\n",
    "ùõø\n",
    "‚ãÖ\n",
    "input\n",
    "‚àÇw\n",
    "‚àÇL\n",
    "\t‚Äã\n",
    "\n",
    "=Œ¥‚ãÖinput.\n",
    "\n",
    "Because you accumulate errors with old weights before modifying them, the gradient calculation is consistent with the forward pass.\n",
    "\n",
    "Practical notes, improvements and gotchas\n",
    "\n",
    "Single-sample SGD vs batch\n",
    "\n",
    "This code computes gradients per single input (online / stochastic gradient descent). For batch training you should accumulate gradients over the batch and either average them or scale appropriately before updating. That usually yields more stable training.\n",
    "\n",
    "Loss choice\n",
    "\n",
    "You used MSE. For classification with sigmoid outputs, cross-entropy loss combined with sigmoid is numerically better and often trains faster (no saturation issues and simpler final gradient).\n",
    "\n",
    "Activation choices and initializations\n",
    "\n",
    "Sigmoid saturates. For deeper nets use ReLU (or GELU) in hidden layers and initialize weights properly:\n",
    "\n",
    "Xavier/Glorot initialization for tanh/sigmoid.\n",
    "\n",
    "He initialization for ReLU.\n",
    "\n",
    "Your code will still work with sigmoid but may be slow or sensitive to learning rate.\n",
    "\n",
    "Learning rate tuning\n",
    "\n",
    "The learning rate must be chosen carefully. Consider adding learning-rate schedules or optimizers (momentum, RMSProp, Adam) later.\n",
    "\n",
    "Gradient clipping\n",
    "\n",
    "For stability you can clip gradients per weight (esp. for deeper nets / RNNs).\n",
    "\n",
    "Numerical stability of activation_prime\n",
    "\n",
    "If activation_prime computes derivative by recomputing sigmoid(z) each time, you might recompute expensive terms. You already store neuron.output ‚Äî you can implement sigmoid_prime_from_output(s) = s*(1-s) and reuse neuron.output.\n",
    "\n",
    "Weight update order\n",
    "\n",
    "You update weights in-place while you accumulate new_errors with old weights ‚Äî which is correct. If you changed this order, you'd need to copy weights.\n",
    "\n",
    "Performance\n",
    "\n",
    "Python lists and nested loops are slow. Later we can vectorize (your own mini-array) or compile critical parts. But for learning principles this is fine.\n",
    "\n",
    "Regularization\n",
    "\n",
    "Consider L2 weight decay (add - learning_rate * lambda * w), dropout in hidden layers, etc.\n",
    "\n",
    "Bias update\n",
    "\n",
    "You used neuron.bias -= learning_rate * error, which is correct since ‚àÇL/‚àÇb = Œ¥.\n",
    "\n",
    "Shape bugs to watch for\n",
    "\n",
    "Make sure every neuron in a layer has the same len(neuron.inputs) (consistent architecture).\n",
    "\n",
    "If you later do batched forward, .inputs must store the full input vector for that sample; for batch you'd store inputs per-sample or accumulate gradients accordingly.\n",
    "\n",
    "Vectorization and cleaner code\n",
    "\n",
    "You can simplify by making Layer support vector ops: keep weight matrix and bias vector and perform a matrix‚Äìvector multiply to compute all z's at once. That simplifies code and improves speed.\n",
    "\n",
    "Edge-case: multiple outputs\n",
    "\n",
    "Your initial dL_dy = 2 * (neuron.output - target[i]) assumes MSE where targets and outputs are aligned lists. Works fine for multiple outputs.\n",
    "\n",
    "Complexity / runtime\n",
    "\n",
    "Per sample, time complexity ‚âà \n",
    "ùëÇ\n",
    "(\n",
    "#weights\n",
    ")\n",
    "O(#weights) (each weight is touched for gradient accumulation and update).\n",
    "\n",
    "Memory is dominated by storing activations and inputs per neuron (for backprop).\n",
    "\n",
    "Suggested immediate small improvements (I can implement these for you)\n",
    "\n",
    "Reuse neuron.output for activation_prime to avoid recomputing activation inside derivative functions:\n",
    "\n",
    "If you store neuron.output = f(z) then sigmoid_prime_from_output = lambda s: s*(1-s).\n",
    "\n",
    "Make Layer expose weight matrices so you can do matrix‚Äìvector ops for speed and clarity.\n",
    "\n",
    "Add a train() wrapper that loops over data, supports minibatches, logs loss, and optionally shuffles data.\n",
    "\n",
    "Add predict() and evaluate() helpers.\n",
    "\n",
    "Add gradient checking (numerical derivative) for small networks to verify backprop is correct.\n",
    "\n",
    "Switch final-layer loss to cross-entropy when doing classification: L = -[ t*log(y) + (1-t)*log(1-y) ] and gradient is simpler with sigmoid.\n",
    "\n",
    "Which direction is best next?\n",
    "\n",
    "You said ‚Äúthen we will go forward.‚Äù I recommend the next step be one of these two (in order of what helps most):\n",
    "\n",
    "A. Build a small autograd / computation-graph engine so you don‚Äôt manually derive gradients for every new op. This will let you write layers cleanly and automatically compute gradients for arbitrary architectures (good for extending to CNNs and Transformers).\n",
    "B. Refactor the current code to vectorized layer-wise matrix operations and add minibatching, Xavier init, and an Adam optimizer ‚Äî faster and more practical for training real examples.\n",
    "\n",
    "Both are important; I suggest doing autograd first because it removes the tedium of manual gradients and makes future experimentation much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c43aa9",
   "metadata": {},
   "source": [
    "## Computation grapgh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d492d",
   "metadata": {},
   "source": [
    "# üß© What is a Computation Graph?\n",
    "\n",
    "A computation graph (also called a computational graph) is a directed acyclic graph (DAG) that represents how a function is computed ‚Äî\n",
    "i.e. how the final output (like loss) depends on inputs (like weights, biases, data).\n",
    "\n",
    "Each node = an operation or a value\n",
    "Each edge = data dependency (what depends on what)\n",
    "\n",
    "## üîπ Example 1 ‚Äî Simple equation\n",
    "\n",
    "Say you compute:\n",
    "\n",
    "z = (x + y) * 2\n",
    "\n",
    "Let's represent it step-by-step:\n",
    "\n",
    "| Step | Expression | Meaning |\n",
    "|------|------------|---------|\n",
    "| 1    | a = x + y  | add inputs |\n",
    "| 2    | z = a * 2  | multiply by constant |\n",
    "\n",
    "\n",
    "\n",
    "## üîπ Example 2 ‚Äî Another equation\n",
    " \n",
    "Let's compute: f = (a * b) + c\n",
    " \n",
    "Forward pass:\n",
    "\n",
    "| Step | Formula  | Value (if a=2, b=3, c=4) |\n",
    "|------|----------|--------------------------|\n",
    "| 1    | d = a * b| 6                        |\n",
    "| 2    | f = d + c| 10                       |\n",
    "\n",
    "\n",
    "üîπ Now the backward pass (gradients)\n",
    "\n",
    "We want derivatives:\n",
    "\n",
    "‚àÇf/‚àÇa, ‚àÇf/‚àÇb, ‚àÇf/‚àÇc\n",
    "\n",
    "By chain rule:\n",
    "\n",
    "‚àÇf/‚àÇd = 1\n",
    "\n",
    "‚àÇd/‚àÇa = b\n",
    "\n",
    "‚àÇd/‚àÇb = a\n",
    "\n",
    "So:\n",
    "\n",
    "| Variable | Derivative                      |\n",
    "|----------|----------------------------------|\n",
    "| ‚àÇf/‚àÇa    | = ‚àÇf/‚àÇd * ‚àÇd/‚àÇa = 1 * b = 3     |\n",
    "| ‚àÇf/‚àÇb    | = ‚àÇf/‚àÇd * ‚àÇd/‚àÇb = 1 * a = 2     |\n",
    "| ‚àÇf/‚àÇc    | = 1 (since f = d + c)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c7acc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2507e13",
   "metadata": {},
   "source": [
    "## Build an Autograd Engine (like micrograd)\n",
    "\n",
    "Goal:\n",
    "Create a small engine that can:\n",
    "\n",
    "Track computations dynamically (build a computation graph)\n",
    "\n",
    "Perform automatic differentiation (backprop) via .backward()\n",
    "\n",
    "Propagate gradients through the graph automatically\n",
    "\n",
    "üîπ Core Idea\n",
    "\n",
    "Each computation (addition, multiplication, etc.) produces a new Value object that remembers:\n",
    "\n",
    "its data (the actual number)\n",
    "\n",
    "its gradient (.grad)\n",
    "\n",
    "its parents (operands that created it)\n",
    "\n",
    "its operation (so we know how to backpropagate)\n",
    "\n",
    "Then, .backward() performs reverse-mode autodiff:\n",
    "\n",
    "Traverse the graph from output to inputs\n",
    "\n",
    "Apply the chain rule to accumulate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c76bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data                # actual scalar value\n",
    "        self.grad = 0.0                 # d(output)/d(this)\n",
    "        self._backward = lambda: None   # function for backprop\n",
    "        self._prev = set(_children)     # input nodes\n",
    "        self._op = _op                  # operation name (for graph/debug)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "    # ---- basic operations ----\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * 1.0\n",
    "            other.grad += out.grad * 1.0\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert isinstance(power, (int, float))\n",
    "        out = Value(self.data ** power, (self,), f'**{power}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (power * (self.data ** (power - 1))) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ---- non-linearities ----\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        s = 1 / (1 + math.exp(-x))\n",
    "        out = Value(s, (self,), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += s * (1 - s) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ---- backward pass ----\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8d445",
   "metadata": {},
   "source": [
    "# Value Class (The Autograd Engine)\n",
    "\n",
    "| Feature | Verification | Notes |\n",
    "|---------|--------------|-------|\n",
    "| __init__ | Correct | Stores data, initializes grad to $0.0$, sets up _backward (chain rule link), _prev (children), and _op. |\n",
    "| __add__ | Correct | Computes $c = a + b$. The derivatives are $\\frac{\\partial c}{\\partial a} = 1$ and $\\frac{\\partial c}{\\partial b} = 1$. The backward step correctly implements the chain rule: $a.\\text{grad} += \\frac{\\partial c}{\\partial a} \\cdot c.\\text{grad}$. |\n",
    "| __mul__ | Correct | Computes $c = a \\cdot b$. The derivatives are $\\frac{\\partial c}{\\partial a} = b$ and $\\frac{\\partial c}{\\partial b} = a$. The backward step correctly applies the chain rule. |\n",
    "| tanh | Correct | The derivative of $\\tanh(x)$ is $1 - \\tanh^2(x)$. The backward step correctly uses the pre-computed output value $t$ to calculate the local derivative $\\frac{\\partial t}{\\partial x} = 1 - t^2$. |\n",
    "| sigmoid | Correct | The derivative of $\\sigma(x)$ is $\\sigma(x)(1 - \\sigma(x))$. The backward step correctly uses the pre-computed output value $s$ for $\\frac{\\partial s}{\\partial x} = s(1 - s)$. |\n",
    "| __pow__ | Correct | The derivative of $x^n$ is $n x^{n-1}$. The backward step correctly implements this. |\n",
    "| backward() | Correct | Crucial part: It uses a topological sort (build_topo) to ensure nodes are processed in the correct order (output to input), then initializes the output's gradient (self.grad = 1.0), and finally iterates in reverse to run _backward() on every node. This is the core of backpropagation. |\n",
    "| Operator Overloading | Correct | The use of __neg__, __sub__, __radd__, __rmul__ allows for seamless arithmetic with both Value objects and primitive numbers, which is essential for usability. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd053d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2.0000, grad=-6.0000) Value(data=-3.0000, grad=4.0000) Value(data=10.0000, grad=2.0000) Value(data=4.0000, grad=2.0000) Value(data=2.0000, grad=4.0000) Value(data=8.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "d = a * b + c\n",
    "e = Value(2.0)\n",
    "f = d * e\n",
    "f.backward()\n",
    "\n",
    "print(a, b, c, d, e, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fa91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.7071, grad=1.0000)\n",
      "0.9999999999999998 -1.4999999999999996 0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "x1 = Value(2.0)\n",
    "x2 = Value(0.0)\n",
    "w1 = Value(-3.0)\n",
    "w2 = Value(1.0)\n",
    "b = Value(6.8813735870195432)\n",
    "\n",
    "y = (w1*x1 + w2*x2 + b).tanh()\n",
    "y.backward()\n",
    "\n",
    "print(y)   # should be near 1.0\n",
    "print(w1.grad, x1.grad, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cb0a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.7071, grad=1.0000)\n",
      "0.41421356237309503 -0.6213203435596426 0.20710678118654752\n"
     ]
    }
   ],
   "source": [
    "x1 = Value(2.0)\n",
    "x2 = Value(0.0)\n",
    "w1 = Value(-3.0)\n",
    "w2 = Value(1.0)\n",
    "b = Value(6.8813735870195432)\n",
    "\n",
    "y = (w1*x1 + w2*x2 + b).sigmoid()\n",
    "y.backward()\n",
    "\n",
    "print(y)   # should be near 1.0\n",
    "print(w1.grad, x1.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ac56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        # Initialize weights and bias\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Weighted sum + bias\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        print(sz)\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "        print(self.layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x if len(x) > 1 else x[0]\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c83116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 1]\n",
      "[<__main__.Layer object at 0x7f76288ee7b0>, <__main__.Layer object at 0x7f76288ec860>]\n",
      "Initial model params: 9\n",
      "[Value(data=-0.0195, grad=0.0000), Value(data=-0.1758, grad=0.0000), Value(data=0.0000, grad=0.0000), Value(data=0.4946, grad=0.0000), Value(data=-0.5351, grad=0.0000), Value(data=0.0000, grad=0.0000), Value(data=0.3514, grad=0.0000), Value(data=-0.1691, grad=0.0000), Value(data=0.0000, grad=0.0000)]\n"
     ]
    }
   ],
   "source": [
    "# XOR dataset\n",
    "data = [\n",
    "    ([Value(0), Value(0)], Value(0)),\n",
    "    ([Value(0), Value(1)], Value(1)),\n",
    "    ([Value(1), Value(0)], Value(1)),\n",
    "    ([Value(1), Value(1)], Value(0)),\n",
    "]\n",
    "\n",
    "model = MLP(2, [2, 1])\n",
    "print(\"Initial model params:\", len(model.parameters()))\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3afcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.000063\n",
      "Epoch 500: loss=0.000058\n",
      "Epoch 1000: loss=0.000056\n",
      "Epoch 1500: loss=0.000057\n",
      "Epoch 2000: loss=0.000050\n",
      "Epoch 2500: loss=0.000054\n",
      "Epoch 3000: loss=0.000049\n",
      "Epoch 3500: loss=0.000048\n",
      "Epoch 4000: loss=0.000049\n",
      "Epoch 4500: loss=0.000044\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5000):\n",
    "    # Forward pass\n",
    "    ypred = [model(xi) for xi, _ in data]\n",
    "    # print(ypred)\n",
    "    loss = sum((yout - yi) ** 2 for yout, (_, yi) in zip(ypred, data))\n",
    "    # print(loss)\n",
    "    # Backward pass\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent\n",
    "    for p in model.parameters():\n",
    "        p.data -= 0.1 * p.grad\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: loss={loss.data:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83bd5d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7f76288ecb90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b2f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Layer at 0x7f76288ee7b0>, <__main__.Layer at 0x7f76288ec860>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec142dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Neuron at 0x7f76288ef470>, <__main__.Neuron at 0x7f76288ee720>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec9f1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.0195, grad=0.0000), Value(data=-0.1758, grad=0.0000)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].neurons[0].w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc524a1",
   "metadata": {},
   "source": [
    "# MLP(2, [2, 1]) Structure\n",
    "\n",
    "MLP(2, [2, 1]) means:\n",
    "- Input: 2 values (not neurons)\n",
    "- Hidden layer: 2 neurons\n",
    "- Output layer: 1 neuron\n",
    "\n",
    "Total: 3 neurons, 5 nodes (including inputs)\n",
    "\n",
    "Input nodes aren't neurons because they:\n",
    "- Don't have weights/biases\n",
    "- Don't apply activation functions\n",
    "- Are just data placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4dae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "138351bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
